Bank Loan Default Prediction (ML Project)
Predicting whether a customer will default on a loan using machine learning.
This project reuses and extends the folder structure + workflow from my previous Telco Churn ML project.
ğŸ“Œ 1. Project Goal
Build a machine learning pipeline that:
Cleans and preprocesses loan application data
Encodes categorical variables
Splits the dataset into training/testing
Trains multiple ML models
Evaluates model performance
Tunes the probability threshold for business decisions
Target variable:
Status = 1 â†’ Customer defaulted
Status = 0 â†’ Customer paid successfully
Why this matters:
Banks want to reduce financial losses, improve risk assessment, and decide whether to approve or decline a loan application.
ğŸ“‚ 2. Folder Structure
bank-loan-default-ml/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/loan_default.csv
â”‚   â”œâ”€â”€ processed/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_eda.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ train_model.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ logistic_regression.pkl
â”‚   â”œâ”€â”€ random_forest.pkl
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ figures/
â”‚   â””â”€â”€ final_metrics.pdf
â”‚
â””â”€â”€ README.md
This structure allows clean, reusable code for future ML projects.
ğŸ“Š 3. Dataset Overview
The dataset contains 148k loan applications with:
Categorical features (loan type, credit type, gender, co-applicant type)
Numeric features (loan amount, interest rate, income, property value)
Missing values in income, rate_of_interest, and spread columns
Class imbalance (only ~24% defaults)
Basic cleaning steps included:
Dropping ID-like columns
Handling missing values
Separating numerical vs categorical columns
Checking duplicates
âš™ï¸ 4. ML Pipeline (Reusable Template)
The project uses a unified scikit-learn Pipeline, including:
Numerical preprocessing
SimpleImputer(strategy='median')
StandardScaler()
Categorical preprocessing
SimpleImputer(strategy='most_frequent')
OneHotEncoder(handle_unknown='ignore')
Combined with ColumnTransformer, then fed into a model such as:
Logistic Regression
Random Forest
(XGBoost planned next)
This makes the workflow clean, repeatable, and ready for deployment.
ğŸ¤– 5. Models Trained
âœ” Logistic Regression
Performs realistically
ROC-AUC â‰ˆ 0.85
Balanced between precision and recall
Best for interpretability
âš  Random Forest (Overfitting Detected)
Returned unrealistic 100% accuracy
Caused by high-cardinality categorical variables + one-hot encoding
Not reliable without category reduction
(Next) XGBoost
Will provide a more powerful and stable alternative.
ğŸ“ˆ 6. Evaluation Metrics
Metrics used:
Confusion Matrix
Precision, Recall, F1-score
ROC Curve + ROC-AUC
Precisionâ€“Recall Curve
Threshold tuning (0.10 â†’ 0.85)
Why threshold tuning matters
Banks may prefer:
High recall â†’ catch every risky borrower
Even if precision drops (more false alarms)
We evaluated thresholds like 0.25, 0.30, 0.35 to improve default detection.
ğŸ“‰ 7. Key Results
Logistic Regression (Threshold = 0.35)
Recall for default: 0.77
Precision for default: 0.44
Balanced approach, interpretable
Random Forest
Produced perfect scores (overfitting)
Not suitable without feature engineering
ğŸ§  8. What I learned
How to reuse ML project templates
How to separate categorical & numeric preprocessing
Why Random Forest overfits with high-cardinality categorical features
How threshold tuning changes business decisions
How to evaluate with PR/ROC curves
How to design a clean classification workflow
ğŸš€ 9. Next Steps
Implement XGBoost
Add feature importance visualisations
Create a final comparison table
Export a printable PDF report
Push the project cleanly to GitHub
ğŸ™Œ 10. Tools Used
Python
scikit-learn
pandas, numpy
matplotlib
VS Code
Jupyter Notebook


ğŸ‘¤ Author
Muhammad Shaan
MSc Computer Science (Data Analytics)
Carlisle, UK


